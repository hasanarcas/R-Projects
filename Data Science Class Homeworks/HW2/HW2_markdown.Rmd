---
title: <span style="color:red">Applying Logistic Regression after PCA</span>
author: "Hasan Arcas"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message=FALSE,
                      warning=FALSE)
```



### The first thing to do is to include the libraries that we are gone to use:
```{r libraries}
library(tidyverse)
library(lattice)
library(caret)
library(reshape2)
library(ggplot2)
```


### Now we can import our data and analize it:
```{r analysis}
breast_cancer_data <- read.csv("../datasets/breast-cancer-wisconsin.csv")
breast_cancer_data$X <- NULL
breast_cancer_data_numeric <- select(breast_cancer_data, radius_mean:fractal_dimension_worst)
head(breast_cancer_data_numeric)  
```


### We can create a heatmap in order to see the correlation between each feature of our data:
```{r heatmap}
corr_matrix <- round(cor(breast_cancer_data_numeric),2)
melted_corr_matrix <- melt(corr_matrix)
ggplot(melted_corr_matrix, aes(Var1, Var2, fill=value)) +
  guides(x =  guide_axis(angle = 90)) +
  geom_tile()
```

#### <span style="color:red">As we can see from our heatmap, some features are highly correlated with each other.</span>


### Now it's time to prepare our data in order to feed it to a model:
```{r prepare_data}
y = breast_cancer_data %>% 
  select(diagnosis)
X = breast_cancer_data[, -2]

set.seed(42)
test_inds <- createDataPartition(y$diagnosis, p = 0.2, list = F)
train_data <- breast_cancer_data[-test_inds,]
test_data <- breast_cancer_data[test_inds,]
```

### Before applying PCA, we can directly apply logistic regression in order to compare the results:
```{r lr}
fit <- glm(as.factor(diagnosis) ~ .,data = train_data, family = binomial)
predictions <- predict(fit, newdata= test_data)
predictions <- ifelse(predictions > 0 , "M", "B")
mean(test_data$diagnosis == predictions)
```
#### <span style="color:red"> Wow! 95.6% accuracy it's not bad, but can we improve it by applying PCA?</span>


### Now it's time to apply PCA on our data, and after that we will apply logistic regression on that:
```{r pca_lr}
PCA <- prcomp(X, scale. = T, center = T)
pca <- as.data.frame(PCA$x[,1:6])
pca$diagnosis <- y$diagnosis

set.seed(2)
test_inds <- createDataPartition(y$diagnosis, p = 0.2, list = F)
train_data_pca <- pca[-test_inds,]
test_data_pca <- pca[test_inds,]

fit_pca <- glm(as.factor(diagnosis) ~ .,data = train_data_pca, family = binomial)
new_predictions <- predict(fit_pca, newdata= test_data_pca)
new_predictions <- ifelse(new_predictions > 0, "M", "B")
mean(test_data_pca$diagnosis == new_predictions)

```
#### <span style="color:red"> 98.2% accuracy!! Here we can see the importance of doing some preproccessing on our data before feeding it to a regression model (or any other model). </span>












